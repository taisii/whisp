# 音声入力アプリを本気で考える

## 導入
Aqua VoiceやTypelessといった音声認識のサブスク型サービスは入力速度を大幅に向上させて、必要不可欠なものになっている。ただ、AIの課金はもうやめられないので、ここにも月12$などを払いたくないから自作してコストを減らしたい。今回はメニューバー常駐のアプリを作成することを目標として、その構築方法について共有する。

## 音声認識アプリの構成
まずは最近流行りの構成の音声認識のアプリがどのような構成で動いているかを考えた。
流行っている理由は、最後にLLMによる整形が行われて、自然な文章が出てくるからだと思う。Aqua Voiceは画面のコンテキストをとって、音声の出力に反映させているようだし、これは間違い無いだろう。
音声認識アプリは以下の3フェーズに分けられると思う
1. コンテキストの認識
    画面の情報を使用するなど、コンテキストの情報を取得して、ユーザーの状況を理解する。
2. STT（Speak To Text）
    音声の文字起こし。
3. 整形
    文字起こしの情報とコンテキストの情報から、ユーザーが出力したいであろう情報に変更する。フィラー（あのー、えーっと）の除去くらいなら、ルールベースでも可能そうだけど、音声認識で落ちてしまった接続詩や句読点などを補うにはLLMを使って処理させるのが良さそう。

### コンテキストの認識
Aqua Voiceなどのアプリではアクセシビリティの権限が求められるが、この権限によって取得できる情報には以下のようなものがある。
（ここにとってこれる情報の列挙）

これらを効果的に組み込むことで整形処理の精度向上を目的としている。

### SST
SSTはOS内蔵のWhisper APIやDeepgramなど、さまざまなサービスが存在している。ここは自作するのは割に合わないと思うので、いろんなプロバイダのものを比較して、いいものを使いたい
（ここで、主要なプロバイダの紹介と、値段ベースの比較くらいはここでできそう）

### 整形
整形はユーザーが文章を入力し終わった後に走るAPI通信であるから、速度が非常に重要。あとはプロンプトを調整して、行くのが良さそう。

#### 補足
LLMに音声データをそのまま投げて文字起こしさせる方式も試しましたが、重い音声データをユーザーのアクションが終わってから送信して処理する必要があるから待ち時間が非常に長くなったため断念。他のサービスでも音声データをLLMにそのまま入れてることはないと思う。Aqua Voiceだって独自のSTTのAPIを公開してるくらいだし。

## 評価方法
自作の音声認識アプリを作るにあたって、決める必要があるのは以下のことだと思う。
- コストと精度のバランスがいいSTTプロバイダ
- LLMのモデル
- LLMに入れるコンテキスト情報とプロンプト
では、これらを評価するための指標を考えたい。

### STTプロバイダ
ここは音声からどれだけ正確に文字起こしができるかという役割を担っているだけなので、機械的な比較が行える。
WER（word error rate）やCER(???)で判断できると思う。あと、一応音声入力を終えてから、テキストが出力されるまでの時間も評価基準に入れたい。あとは値段

### LLMの出力
ここはユーザーの意図した出力になっているか。というのが一番シンプルな評価基準だと思うけど、それをどうやったら実現できるだろうか。音声とコンテキストの組について、人力で正解となる文章を打ち込むのがいいだろうか。ただ、正解は一つじゃ無いし、正解となる文章との編集距離などで判定してしまうと、意図は組めているのに人力で入力した正解と離れているという理由だけで低評価がつくことになってしまう。
そこで、人力で入力した正解となる文章と、出力の意図をLLMに比べさせて点数をつけさせるのはどうかな。と思った。これなら元々の目的であったユーザーの意図した出力になっているかをある程度忠実に判断できるだろう。ただ、ベンチマークを回すたびにLLMのAPIを呼ばないといけないのは嫌だな。そもそも、人力で入力する正解となる文章についても、LLMに画像、音声、コンテキストなどをすべて与えれば良いものが出せるんじゃないか。そうすればボトルネックになると思われる人力での正解入力を自動化できて良さそうだとおもった。

## 実際の評価
私が実際に使用した時の音声をベンチマークとしてテストを行なった結果、次のようになった。

（後々、実験を回して結果を載せる）

この結果から〇〇という構成が適していそうとわかった。今回の音声認識アプリについてGithubで公開しているのでぜひ使ってみて欲しい。