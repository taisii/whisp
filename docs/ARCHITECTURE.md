# Whisp アーキテクチャ概要

## このドキュメントについて

本ドキュメントは、Whispの設計思想と実現アプローチを説明するものです。技術的な実装詳細ではなく、「何を目指しているか」「なぜそのアプローチを取るか」に焦点を当てています。

---

## 1. 解決したい課題

### 現状の問題

音声入力ツールは存在するが、以下の理由で日常的に使いづらい：

- **遅い**: 話し終わってからテキストが出るまで待たされる
- **不正確**: 技術用語やカタカナが正しく変換されない（例：「リアクト」→「リアクト」のまま）
- **手間がかかる**: 後から手動で修正が必要

### 私たちが目指す姿

**「話すだけで、すぐに使えるテキストが手に入る」体験を作る**

具体的には：
- 話し終わって0.5秒以内にテキストが手に入る
- 「えーと」「あのー」などの不要な言葉は自動で消える
- 技術用語は正しい表記に自動補正される（React, useState, TypeScript など）
- ショートカットキー1つで完結し、操作を邪魔しない

---

## 2. 価値提案

### 誰のためのツールか

- **開発者・エンジニア**: コード中のコメント、ドキュメント作成、チャットでの説明
- **ナレッジワーカー**: メモ、アイデアの記録、メール下書き
- **技術文書を書く人**: 技術用語が多いテキストの入力

### 提供する価値

| 従来の課題 | Whispの解決策 |
|-----------|-------------|
| 入力が遅い | 0.5秒以内のレスポンス |
| 技術用語が変換されない | AI による自動補正 |
| フィラーワードが残る | 自動削除 |
| 操作が煩雑 | ショートカット1つで完結 |
| アプリが邪魔 | メニューバー常駐、画面を占有しない |

---

## 3. 確定仕様

### 基本操作
| 項目 | 仕様 |
|------|------|
| **アプリ名** | Whisp |
| **ショートカット** | Cmd+J（初期値、設定で変更可能） |
| **録音方式** | トグル方式（1回押しで開始、もう1回で終了） |

### UI/UX
| 項目 | 仕様 |
|------|------|
| **アプリ形態** | メニューバーアプリ（常駐） |
| **録音中フィードバック** | アイコン色変化（録音中=赤、待機中=グレー） |
| **完了通知** | サウンドのみ |
| **エラー通知** | macOS通知センター |

### 出力
| 項目 | 仕様 |
|------|------|
| **対応言語** | 日本語 / 英語 / 自動判定 |
| **出力形式** | プレーンテキスト |
| **出力先** | クリップボード（自動ペースト可能） |

### 後処理
| 項目 | 仕様 |
|------|------|
| **フィラー除去** | えーと、あのー、なんか 等 |
| **技術用語補正** | React, useState, TypeScript 等 |
| **句読点** | 自動追加 |

---

## 4. 実現アプローチ

### 4.1 基本的な流れ

```
ユーザーの操作          システムの処理           結果
─────────────────────────────────────────────────────
ショートカットを押す → 録音開始              → アイコンが赤に変わる
話す                → 音声をリアルタイム送信
ショートカットを押す → 録音停止              → 音声認識 → AI補正 → クリップボードへ
                                            → 完了音が鳴る
                                            → 自動でペーストされる（設定により）
```

### 4.2 なぜ2段階の処理か

音声認識だけでは「そのまま聞き取った文字列」しか得られません。人が話す言葉には：

- 「えーと」「なんか」などの無意味な言葉
- 言い直し、言い淀み
- 技術用語のカタカナ読み

これらが含まれます。

**Whispは音声認識の後にAIによる整形処理を行うことで、「使えるテキスト」を出力します。**

```
音声認識の出力:
「えーと、リアクトのユーズステートで、なんか、値を管理したいんですけど」

AI整形後の出力:
「ReactのuseStateで値を管理したい」
```

### 4.3 なぜリアルタイム処理か

従来のアプローチ：録音 → 完了 → 送信 → 認識 → 結果

このアプローチでは、話し終わってから認識処理が始まるため、待ち時間が長くなります。

**Whispのアプローチ：録音しながら同時に送信・認識**

話している間に認識処理が進むため、話し終わった瞬間にはほぼ結果が揃っています。

---

## 5. システム構成の考え方

### 5.1 デスクトップアプリとして作る理由

Webアプリやブラウザ拡張ではなく、デスクトップアプリとして作る理由：

- **グローバルショートカット**: どのアプリを使っていても同じショートカットで起動できる
- **システムクリップボード**: 認識結果をすぐにどのアプリにも貼り付けられる
- **自動ペースト**: キーボード操作をシミュレートして自動入力できる
- **常駐動作**: メニューバーに常駐し、いつでも使える状態を維持

### 5.2 外部サービスを使う理由

音声認識とAI整形は外部サービス（API）を利用します。

**音声認識：Deepgram**
- 高速かつ高精度な音声認識
- リアルタイムストリーミングに対応
- 日本語・英語に対応

**AI整形：Google Gemini**
- 高速なレスポンス
- 文脈を理解した自然な整形
- 技術用語の知識

自前で構築するより、専門サービスを組み合わせることで：
- 開発・運用コストを抑える
- 最新の技術をすぐに利用できる
- 精度向上の恩恵を自動的に受けられる

### 5.3 プライバシーへの配慮

- 音声データは認識処理のためだけに送信され、保存されない
- 認識履歴はローカルに保存しない（デフォルト設定）
- APIキーはユーザー自身が管理

---

## 6. ユーザー体験の設計

### 6.1 最小限のUI

Whispは「見えないツール」を目指しています。

- **メニューバーアイコンのみ**: ウィンドウを開かずに使える
- **状態はアイコン色で表示**: 灰色（待機中）、赤（録音中）
- **設定画面は必要な時だけ**: APIキーの設定など初期設定のみ

### 6.2 フィードバックの設計

操作に対する応答を適切に返すことで、安心感を提供：

| 操作 | フィードバック |
|-----|--------------|
| 録音開始 | アイコンが赤に変わる |
| 録音終了 | アイコンが灰色に戻る |
| 処理完了 | 完了音が鳴る |
| エラー発生 | 通知で知らせる |

### 6.3 カスタマイズ可能な部分

- ショートカットキーの割り当て
- 自動ペーストのON/OFF
- 認識言語の選択（自動検出/日本語/英語）

---

## 7. 成功指標

### 定量的な目標

| 指標 | 目標値 |
|-----|-------|
| 総レイテンシ（話し終わり→クリップボード） | 500ms以下 |
| 認識精度（意図した内容が伝わる率） | 90%以上 |
| 技術用語の正しい変換率 | 95%以上 |

### 定性的な目標

- 「キーボードで打つより速い」と感じられる
- 「認識結果をそのまま使える」と感じられる
- 「存在を忘れるくらい自然に使える」と感じられる

---

## 8. 今後の拡張可能性

現在のアーキテクチャで将来的に対応可能な拡張：

- **辞書機能**: ユーザー固有の用語を登録
- **コンテキスト認識**: 使用中のアプリに応じた変換ルール
- **多言語対応**: より多くの言語への対応
- **オフライン対応**: ローカルモデルによる認識（精度とのトレードオフ）

---

## 9. 議論ポイント

このアーキテクチャについて、以下の点について議論を深めたい：

1. **ターゲットユーザーの優先順位**: 開発者向けに特化するか、より広い層を狙うか
2. **精度と速度のバランス**: より高精度な処理を追加するとレイテンシが増える
3. **プライバシーと利便性**: ローカル処理への移行は精度低下を伴う可能性がある
4. **収益モデル**: 外部API利用のコストをどう賄うか

---

## 用語集

| 用語 | 説明 |
|-----|-----|
| レイテンシ | 操作から結果が得られるまでの遅延時間 |
| STT (Speech-to-Text) | 音声をテキストに変換する技術 |
| フィラーワード | 「えーと」「あのー」など、意味を持たない発話 |
| グローバルショートカット | どのアプリを使っていても反応するキーボードショートカット |
| API | 外部サービスの機能を利用するための接続方法 |
