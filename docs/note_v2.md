# 音声入力アプリの自作で考えたこと — 技術選定と評価設計の話

## 導入

Aqua VoiceやTypelessといった音声入力サービスが注目を集めている。これらのサービスは単なる文字起こしではなく、フィラーの除去や句読点の補完、文脈に応じた用語の修正まで行い、「話すだけで自然な文章が出てくる」体験を提供している。

便利だが、月額12ドル前後のサブスクリプションが必要になる。LLMのAPIやその他の開発ツールへの課金が積み重なる中、ここも自作でコストを抑えられないかと考えた。

この記事では、macOSのメニューバーに常駐する音声入力アプリを自作した過程を通じて、以下の2点を共有する。

- **STTプロバイダの比較選定**：どのプロバイダが精度・速度・コストのバランスに優れるか
- **コンテキスト情報の活用と、その出力をどう評価するか**：画面情報やカーソル周辺のテキストを使うとどれだけ品質が上がるのか、そしてその「品質」をどう測るのか
確かに画像だけだと、あまり良くないような気がしてきました。

## 音声入力アプリの仕組み

最近の音声入力アプリは、大きく3つのフェーズで動いている。

```
① コンテキスト認識 → ② 音声の文字起こし（STT） → ③ LLMによる整形
```

**① コンテキスト認識** — ユーザーが今何をしているかを把握する。後段の整形処理の精度を高めるために、以下の2系統の情報を収集する。

- **アクセシビリティ情報**：macOSのアクセシビリティAPIを通じて、フォーカス中の要素（テキストフィールド、エディタなど）から、カーソル周辺のテキスト（前後約180文字）や選択中のテキストを取得する。Aqua Voiceなどのアプリがアクセシビリティ権限を要求するのはこのためだ。
- **スクリーンショット解析（Vision）**：画面のスクリーンショットをLLMに渡し、画面の要約（1行）と画面上に見える専門用語（最大10語）をJSON形式で抽出する。これにより、アクセシビリティAPIだけでは拾えない視覚的な文脈を補完できる。

これらは最終的に「選択テキスト」「画面の要約」「専門用語候補」としてプロンプトに注入される。

**② STT（Speech to Text）** — 音声をテキストに変換する。Deepgram、Google、OS内蔵のWhisperなど、多くのプロバイダが存在する。リアルタイム性が求められるため、WebSocketベースのストリーミング方式が主流。

**③ LLMによる整形** — 文字起こし結果をLLMに通し、フィラー（「えーと」「あのー」）の除去、句読点の補完、誤認識された用語の修正を行う。コンテキスト情報と合わせることで、ユーザーが入力したかったであろう文章に近づける。

なお、「LLMに音声データをそのまま投げて文字起こしから整形まで一括で処理させる」方式も試したが、音声データのサイズが大きいため、ユーザーの発話終了後の待ち時間が長くなり断念した。STTとLLM整形を分離するのは、実用上のレイテンシ要件からくる必然的な設計判断だと考えている。

## 問題設定：何を決める必要があるか

アプリの構成が決まったところで、具体的に選択・最適化すべき変数を整理する。

| 決定変数 | 選択肢の例 | 評価の観点 |
|---|---|---|
| STTプロバイダ | Deepgram, Google, OS内蔵Whisper | 精度、レイテンシ、コスト |
| LLMモデル | Gemini 2.5 Flash Lite, GPT-4o-mini, GPT-5 Nano | 出力品質、速度、コスト |
| コンテキスト情報 | アクセシビリティ情報、スクリーンショット解析 | 整形精度への寄与度 |
| プロンプト設計 | テンプレート構成、ルール記述 | 出力の安定性と品質 |

これらの組み合わせの中から、**コストとユーザー体験（精度 × 速度）のバランスが最も良い構成**を見つけるのがゴールとなる。

ただし、4つの変数すべてを同列に扱うと話が発散する。そこで本記事では以下の2点に焦点を絞る。

1. **STTプロバイダの選定** — どのプロバイダが精度・速度・コストのバランスに優れるか。これはプロバイダを付け替えて機械的に比較できる領域であり、別途実験を行った。
2. **コンテキスト情報の活用と出力評価** — 収集したコンテキストをどうLLMに渡すかによって最終出力がどう変わるか。そしてその出力の品質をどう測るか。

LLMモデルについては、簡易な計測でGemini 2.5 Flash Liteが応答速度とコストの面で優れていたため、これを採用した。プロンプト設計はコンテキスト活用の一部として扱う。

ここで難しいのは、2つ目の「出力の品質をどう測るか」だ。STTの精度は機械的に測れるが、最終出力の品質——つまり「ユーザーが意図した文章になっているか」——はどう測ればよいだろうか。

## 評価設計：正解のない出力をどう測るか

### STT：機械的に測れる領域

STTの評価は比較的単純だ。音声に対する正解テキストを用意すれば、以下の指標で機械的に比較できる。

- **CER（Character Error Rate）**：文字単位の誤り率。日本語のように単語境界が曖昧な言語ではWER（Word Error Rate）よりこちらが適している
- **レイテンシ**：発話終了から文字起こし完了までの時間。ストリーミング方式とREST方式で大きく異なる
- **コスト**：1分あたりの単価（例：Deepgramは約$0.0077/分）

これらは数値として比較可能なので、ベンチマークスクリプトで自動計測し、プロバイダ間の比較を行った。

### コンテキスト活用：何を渡すと出力が良くなるのか

STTプロバイダが決まれば、次の問いは「コンテキスト情報をどう活用すれば出力品質が上がるか」だ。

前述の通り、コンテキストには「アクセシビリティ情報（カーソル周辺テキスト）」と「Vision情報（画面の要約・専門用語）」の2系統がある。これらをプロンプトに注入するかしないか、どの組み合わせで渡すかによって、整形後の出力品質は変わるはずだ。

たとえば、プログラミング中に「リアクトのユーズステートで」と発話した場合、画面にReactのコードが映っていれば、Visionから「React」「useState」が専門用語として抽出され、正しく「ReactのuseStateで」と整形できる可能性が高まる。逆に、コンテキストなしでは「リアクトの湯瀬テートで」のような誤変換が残りかねない。

このコンテキストの有無・組み合わせによる品質差を定量的に測りたい。しかし、ここで本質的な問題にぶつかる——最終出力の「品質」をどう定義し、どう測るのか。

### LLM出力の評価：正解が一つではない問題

難しいのはLLMによる整形後の最終出力の評価だ。

たとえば「えーと、昨日のミーティングで、あの、新しいAPIの仕様について話したんですけど」という音声入力に対して、以下はどれも「正解」と言える。

- 「昨日のミーティングで新しいAPIの仕様について話しました。」
- 「昨日のミーティングで、新しいAPIの仕様について話した。」
- 「昨日のMTGで新しいAPI仕様の話をした。」

人力で正解テキストを用意し、編集距離（CERなど）で機械的に比較する方法はすぐに思いつく。しかし、この方法では「意図は正しく汲めているが、表現が正解テキストと異なる」だけで低評価になってしまう。

#### アプローチ：LLM-as-a-Judge

そこで採用したのが、**LLMに出力の品質を判定させる**アプローチだ。

具体的には、各テストケースに対して「このケースでユーザーが意図していた内容」をラベルとして用意し、LLMに「出力がユーザーの意図と合致しているか」を判定させる。これにより、表現の揺れに影響されずに意図の一致度を評価できる。

```
[入力]
- 音声認識結果（STT出力）
- コンテキスト情報
- ユーザーの意図ラベル（正解）

[評価]
LLMが「出力がユーザーの意図と合致しているか」を判定
```

この方法の利点は、文字列の完全一致に依存しないため、表現の多様性を許容しながら品質を評価できる点にある。

#### 正解データ生成の自動化

もう一つの課題は、正解データ（意図ラベル）の作成コストだ。テストケースごとに人力で正解を書くのはスケールしない。

ここで考えたのが、**正解データの生成自体もLLMに任せる**という方法だ。音声、スクリーンショット、アクセシビリティ情報など、利用可能なすべてのコンテキストをLLMに与えれば、「ユーザーが意図していたであろう文章」を高精度に推定できる。実際のアプリではコストやレイテンシの制約から使えるコンテキストを絞っているが、ベンチマーク用の正解生成ではそうした制約がないため、リッチな入力で高品質な正解を作れる。

（TODO: この方法の妥当性について、もう少し掘り下げる。正解生成に使うLLMと評価に使うLLMを同じにして良いのか、など）

### 評価の全体像

最終的に、以下のような多層的な評価体系を構築した。

| 評価レイヤー | 対象 | 主な指標 |
|---|---|---|
| STT単体 | 音声 → テキスト | CER、レイテンシ |
| STTレイテンシ比較 | REST vs ストリーミング | post_stop_latency、finalize時間 |
| コンテキスト抽出 | スクリーンショット → 要約・用語 | 抽出精度 |
| 生成品質 | STT + コンテキスト → 最終テキスト | intent_match_rate、CER |
| E2Eパイプライン | 音声 → 最終テキスト | 総合精度、総レイテンシ |

レイヤーを分けたのは、実際に改善を回す中での必要性からだ。E2Eで精度が低いとわかっても、STTが誤認識しているのか、LLMの整形で崩れたのか、コンテキスト抽出が的外れだったのかで打つ手がまったく違う。各レイヤーを独立に計測できるようにしておかないと、改善の手がかりがつかめなかった。

## 実験結果と考察

### STTプロバイダの比較

（TODO: プロバイダ別のCER・レイテンシ・コストの比較結果を記載する）

### コンテキストの有無・組み合わせによる出力品質の差

（TODO: コンテキストなし / アクセシビリティのみ / Visionのみ / 両方あり の4条件でintent_match_rateとCERを比較した結果を記載する）

## まとめ

（TODO: 実験結果を踏まえた最適構成と、得られた知見をまとめる）
